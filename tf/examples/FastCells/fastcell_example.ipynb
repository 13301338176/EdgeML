{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastRNN and FastGRNN in Tensorflow\n",
    "\n",
    "This is a simple notebook that illustrates the usage of Tensorflow implementation of FastRNN and FastGRNN. We are using the USPS dataset. Please refer to `fetch_usps.py` and run it for downloading and cleaning up the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "# Licensed under the MIT license.\n",
    "\n",
    "import helpermethods\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.insert(0, '../../')\n",
    "\n",
    "#Provide the GPU number to be used\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] =''\n",
    "\n",
    "#FastRNN and FastGRNN imports\n",
    "from edgeml.trainer.fastTrainer import FastTrainer\n",
    "from edgeml.graph.rnn import FastGRNNCell\n",
    "from edgeml.graph.rnn import FastRNNCell\n",
    "\n",
    "# Fixing seeds for reproducibility\n",
    "tf.set_random_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USPS Data\n",
    "\n",
    "It is assumed that the USPS data has already been downloaded and set up with the help of [fetch_usps.py](fetch_usps.py) and is present in the `./usps10` subdirectory.\n",
    "\n",
    "Note: Even though usps10 is not a time-series dataset, it can be assumed as, a time-series where each row is coming in at one single time.\n",
    "So the number of timesteps = 16 and inputDims = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading and Pre-processing dataset for FastCells\n",
    "dataDir = \"usps10/\"\n",
    "(dataDimension, numClasses, Xtrain, Ytrain, Xtest, Ytest) = helpermethods.preProcessData(dataDir)\n",
    "print(\"Feature Dimension: \", dataDimension)\n",
    "print(\"Num classes: \", numClasses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Parameters\n",
    "\n",
    "FastRNN and FastGRNN work for most of the hyper-parameters with which you could acheive decent accuracies on LSTM/GRU. Over and above that, you can use low-rank, sparsity and quatization to reduce model size upto 45x when compared to LSTM/GRU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = \"FastGRNN\" # Choose between FastGRNn & FastRNN\n",
    "\n",
    "inputDims = 16 #features taken in by RNN in one timestep\n",
    "hiddenDims = 32 #hidden state of RNN\n",
    "\n",
    "totalEpochs = 300\n",
    "batchSize = 100\n",
    "\n",
    "learningRate = 0.01\n",
    "decayStep = 200\n",
    "decayRate = 0.1\n",
    "\n",
    "outFile = None #provide your file, if you need all the logging info in a file\n",
    "\n",
    "#low-rank parameterisation for weight matrices. None => Full Rank\n",
    "wRank = None \n",
    "uRank = None \n",
    "\n",
    "#Sparsity of the weight matrices. x => 100*x % are non-zeros\n",
    "sW = 1.0 \n",
    "sU = 1.0\n",
    "\n",
    "#Non-linearities for the RNN architecture. Can choose from \"tanh, sigmoid, relu, quantTanh, quantSigm\"\n",
    "update_non_linearity = \"tanh\"\n",
    "gate_non_linearity = \"sigmoid\"\n",
    "\n",
    "assert dataDimension % inputDims == 0, \"Infeasible per step input, Timesteps have to be integer\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Placeholders for Data feeding during training and infernece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(\"float\", [None, int(dataDimension / inputDims), inputDims])\n",
    "Y = tf.placeholder(\"float\", [None, numClasses])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a directory for current model in the datadirectory using timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "currDir = helpermethods.createTimeStampDir(dataDir, cell)\n",
    "helpermethods.dumpCommand(sys.argv, currDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastCell Graph Object\n",
    "\n",
    "Instantiating the FastCell Graph using modular RNN Cells which will be used for training and inference.\n",
    "\n",
    "Note: RNN cells in edgeml.rnn can be used anywhere in place of LSTM/GRU in a plug & play fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create appropriate RNN cell object based on choice\n",
    "if cell == \"FastGRNN\":\n",
    "    FastCell = FastGRNNCell(hiddenDims, gate_non_linearity=gate_non_linearity,\n",
    "                            update_non_linearity=update_non_linearity,\n",
    "                            wRank=wRank, uRank=uRank)\n",
    "elif cell == \"FastRNN\":\n",
    "    FastCell = FastRNNCell(hiddenDims, update_non_linearity=update_non_linearity,\n",
    "                           wRank=wRank, uRank=uRank)\n",
    "else:\n",
    "    sys.exit('Exiting: No Such Cell as ' + cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastCell Trainer Object\n",
    "\n",
    "Instantiating the FastCell Trainer which will be used for 3 phase training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastCellTrainer = FastTrainer(FastCell, X, Y, sW=sW, sU=sU, learningRate=learningRate, outFile=outFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Session declaration and variable initialization. Interactive Session doesn't clog the entire GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastCell Training Routine\n",
    "\n",
    "The method to to run the 3 phase training, followed by giving out the best early stopping model, accuracy along with saving of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastCellTrainer.train(batchSize, totalEpochs, sess, Xtrain, Xtest,\n",
    "                      Ytrain, Ytest, decayStep, decayRate, dataDir, currDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If one uses piece-wise linear approximations for non-linearities liek quant Tanh for tanh and quantSigm for Sigmoid, they can benefit greatly from pure integer arithmetic after model quantization during prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model quantization\n",
    "model_dir = \"\" #you wil lsee model dir printed at the end of trianing\n",
    "\n",
    "import quantizeFastModels\n",
    "quantizeFastModels.quantizeFastModels(model_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
