{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using EMI-RNN on the HAR Dataset\n",
    "\n",
    "This is a very simple example of how the existing EMI-RNN implementation can be used on the HAR dataset. We illustrate how to train a model that predicts on 48 step sequence in place of the 128 length baselines while attempting to predict early. For more advanced use cases which involves more sophisticated computation graphs or loss functions, please refer to the doc strings provided with the released code.\n",
    "\n",
    "In the preprint of our work, we use the terms *bag* and *instance* to refer to the LSTM input sequence of original length and the shorter ones we want to learn to predict on, respectively. In the code though, *bag* is replaced with *instance* and *instance* is replaced with *sub-instance*. To avoid ambiguity, we will use the terms *bag* and *sub-instance*  throughout this document.\n",
    "\n",
    "The network used here is a simple LSTM + Linear classifier network. \n",
    "\n",
    "The UCI [Human Activity Recognition](https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-23T13:56:24.498625Z",
     "start_time": "2018-07-23T13:56:23.585261Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "sys.path.insert(0, '../')\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] =''\n",
    "\n",
    "# MI-RNN and EMI-RNN imports\n",
    "from edgeml.graph.rnn import EMI_DataPipeline\n",
    "from edgeml.graph.rnn import EMI_BasicLSTM\n",
    "from edgeml.trainer.emirnnTrainer import EMI_Trainer, EMI_Driver\n",
    "import edgeml.utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us set up some network parameters for the computation graph. These will be explained later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-23T13:56:24.511517Z",
     "start_time": "2018-07-23T13:56:24.500664Z"
    }
   },
   "outputs": [],
   "source": [
    "# Network parameters for our LSTM + FC Layer\n",
    "NUM_HIDDEN = 16\n",
    "NUM_TIMESTEPS = 48\n",
    "NUM_FEATS = 9\n",
    "FORGET_BIAS = 1.0\n",
    "NUM_OUTPUT = 6\n",
    "USE_DROPOUT = False\n",
    "KEEP_PROB = 0.9\n",
    "\n",
    "# For dataset API\n",
    "PREFETCH_NUM = 5\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Number of epochs in *one iteration*\n",
    "NUM_EPOCHS = 5\n",
    "# Number of iterations in *one round*. After each iteration,\n",
    "# the model is dumped to disk. At the end of the current\n",
    "# round, the best model among all the dumped models in the\n",
    "# current round is picked up..\n",
    "NUM_ITER = 3\n",
    "# A round consists of multiple training iterations and a belief\n",
    "# update step using the best model from all of these iterations\n",
    "NUM_ROUNDS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data\n",
    "Please download the UCI datset from the above link and use your favorite data loading methods to set up (`x_train`, `y_train`) and (`x_val`, `y_val`) numpy arrays.\n",
    "\n",
    "### Data Preparation\n",
    "\n",
    "[Typical RNN models](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/recurrent_network.ipynb) by convention, use a 3 dimensional tensor for the input data. This tensor is of shape `[number of examples, number of time steps, number of features]`. To incorporate the notion of *bags* and *sub-instances*, we extend this by adding an additional fourth dimension, thus making our input data shape - `[number of bags, number of sub-instances, number of time steps, number of features]`. Additionally, the typical shape of the one-hot encoded label tensor - `[number of examples, number of outputs]` is extended to incorporate sub-instance level labels, thus making it `[number of bags, number of sub-instances, number of output classes]`.\n",
    "\n",
    "Specifically for HAR dataset, the data creation algorithm looks something like this.\n",
    "\n",
    "```\n",
    "def createData(X, Y, subinstanceWidth, subinstanceStride):\n",
    "    '''\n",
    "    TODO: Provide actual code\n",
    "    \n",
    "    Here X and Y are time series input from HAR and their labels. This methods\n",
    "    chops the sequences into temporarily ordered set of sub-instances. All \n",
    "    sub-instances are given the same label as the bag.\n",
    "    '''\n",
    "    assert len(X) == len(Y)\n",
    "    assert len(X.shape) == 3\n",
    "    assert len(Y.shape) == 2\n",
    "    \n",
    "    X_out = []\n",
    "    Y_out = []\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        bag = X[i]\n",
    "        bagLabel = Y[i]\n",
    "        \n",
    "        instances = breakBagIntoInstances(bag, subinstanceWidth, subinstaceStride)\n",
    "        instanceLabels = [Y[i]] * len(instances)\n",
    "        X_out.append(instances)\n",
    "        Y_out.append(instanceLabels)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-23T13:56:24.587027Z",
     "start_time": "2018-07-23T13:56:24.513254Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape is: (6220, 6, 48, 9)\n",
      "y_train shape is: (6220, 6, 6)\n",
      "x_test shape is: (1132, 6, 48, 9)\n",
      "y_test shape is: (1132, 6, 6)\n"
     ]
    }
   ],
   "source": [
    "# Loading the data and shit\n",
    "x_train, y_train = np.load('./HAR/48_16/x_train.npy'), np.load('./HAR/48_16/y_train.npy')\n",
    "x_test, y_test = np.load('./HAR/48_16/x_test.npy'), np.load('./HAR/48_16/y_test.npy')\n",
    "x_val, y_val = np.load('./HAR/48_16/x_val.npy'), np.load('./HAR/48_16/y_val.npy')\n",
    "\n",
    "# BAG_TEST, BAG_TRAIN, BAG_VAL represent bag_level labels. These are used for the label update\n",
    "# step of EMI/MI RNN\n",
    "BAG_TEST = np.argmax(y_test[:, 0, :], axis=1)\n",
    "BAG_TRAIN = np.argmax(y_train[:, 0, :], axis=1)\n",
    "BAG_VAL = np.argmax(y_val[:, 0, :], axis=1)\n",
    "NUM_SUBINSTANCE = x_train.shape[1]\n",
    "print(\"x_train shape is:\", x_train.shape)\n",
    "print(\"y_train shape is:\", y_train.shape)\n",
    "print(\"x_test shape is:\", x_val.shape)\n",
    "print(\"y_test shape is:\", y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computation Graph\n",
    "\n",
    "The *EMI-RNN* computation graph consists of three parts:\n",
    "1. `EMI_DataPipeline`: A data input pipeline that uses the Tensorflow Dataset API. This helps us implement an efficient data input pipeline.\n",
    "2. `EMI_RNN`: An implementation of the abstract `EMI_RNN` class, for instance, `EMI_LSTM`,  which defines the forward computation or inference graph, and\n",
    "3. `EMI_Trainer`: An instance of `EMI_Trainer` class which defines the loss functions and the training graph.\n",
    "\n",
    "To build the computation graph, we create an instance of all the above and then connect them together.\n",
    "\n",
    "The `EMI_BasicLSTM` class is an implementation that uses an LSTM cell and provides the LSTM output at each step for a secondary classifier to use. This secondary classifier is not implemented as part of `EMI_BasicLSTM` and is left to the user to define. The secondary classifier is define by overriding the `createExtendedGraph` method, and the `restoreExtendedgraph` method.\n",
    "\n",
    "For the purpose of this example, lets use a simple linear secondary classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-23T13:56:24.604449Z",
     "start_time": "2018-07-23T13:56:24.588879Z"
    }
   },
   "outputs": [],
   "source": [
    "def createExtendedGraph(self, baseOutput, *args, **kwargs):\n",
    "    W1 = tf.Variable(np.random.normal(size=[NUM_HIDDEN, NUM_OUTPUT]).astype('float32'), name='W1')\n",
    "    B1 = tf.Variable(np.random.normal(size=[NUM_OUTPUT]).astype('float32'), name='B1')\n",
    "    y_cap = tf.add(tf.tensordot(baseOutput, W1, axes=1), B1, name='y_cap_tata')\n",
    "    self.output = y_cap\n",
    "    self.graphCreated = True\n",
    "\n",
    "def restoreExtendedGraph(self, graph, *args, **kwargs):\n",
    "    y_cap = graph.get_tensor_by_name('y_cap_tata:0')\n",
    "    self.output = y_cap\n",
    "    self.graphCreated = True\n",
    "    \n",
    "EMI_BasicLSTM._createExtendedGraph = createExtendedGraph\n",
    "EMI_BasicLSTM._restoreExtendedGraph = restoreExtendedGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-23T13:56:24.680121Z",
     "start_time": "2018-07-23T13:56:24.606138Z"
    }
   },
   "outputs": [],
   "source": [
    "inputPipeline = EMI_DataPipeline(NUM_SUBINSTANCE, NUM_TIMESTEPS, NUM_FEATS, NUM_OUTPUT)\n",
    "emiLSTM = EMI_BasicLSTM(NUM_SUBINSTANCE, NUM_HIDDEN, NUM_TIMESTEPS, NUM_FEATS,\n",
    "                        forgetBias=FORGET_BIAS, useDropout=USE_DROPOUT)\n",
    "emiTrainer = EMI_Trainer(NUM_TIMESTEPS, NUM_OUTPUT, lossType='xentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the elementary parts of the computation graph setup, we connect them together to form the forward graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-23T13:56:28.011492Z",
     "start_time": "2018-07-23T13:56:24.681788Z"
    }
   },
   "outputs": [],
   "source": [
    "# ... for good measure\n",
    "np.random.seed(42)\n",
    "tf.reset_default_graph()\n",
    "g1 = tf.Graph()    \n",
    "with g1.as_default():\n",
    "    x_batch, y_batch = inputPipeline()\n",
    "    y_cap = emiLSTM(x_batch)\n",
    "    emiTrainer(y_cap, y_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMI Driver\n",
    "\n",
    "The `EMI_Driver` implements the `EMI_RNN` algorithm. TODO: Explain \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-23T13:59:43.877276Z",
     "start_time": "2018-07-23T13:56:28.013640Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round: 0\n",
      "Epoch   4 Batch   180 (  960) Loss 0.00562 Acc 0.88021 | Val acc 0.92665 | Model saved to /tmp/model, global_step 1000\n",
      "Epoch   4 Batch   180 (  960) Loss 0.00518 Acc 0.89062 | Val acc 0.93547 | Model saved to /tmp/model, global_step 1001\n",
      "Epoch   4 Batch   180 (  960) Loss 0.00455 Acc 0.89062 | Val acc 0.94300 | Model saved to /tmp/model, global_step 1002\n",
      "INFO:tensorflow:Restoring parameters from /tmp/model-1002\n",
      "Round: 1\n",
      "Epoch   4 Batch   180 (  960) Loss 0.00464 Acc 0.89062 | Val acc 0.94010 | Model saved to /tmp/model, global_step 1003\n",
      "Epoch   4 Batch   180 (  960) Loss 0.00280 Acc 0.91667 | Val acc 0.93345 | Model saved to /tmp/model, global_step 1004\n",
      "Epoch   4 Batch   180 (  960) Loss 0.00235 Acc 0.92708 | Val acc 0.93302 | Model saved to /tmp/model, global_step 1005\n",
      "INFO:tensorflow:Restoring parameters from /tmp/model-1003\n",
      "Round: 2\n",
      "Epoch   4 Batch   180 (  960) Loss 0.00280 Acc 0.91667 | Val acc 0.93345 | Model saved to /tmp/model, global_step 1006\n",
      "Epoch   4 Batch   180 (  960) Loss 0.00235 Acc 0.92708 | Val acc 0.93302 | Model saved to /tmp/model, global_step 1007\n",
      "Epoch   4 Batch   180 (  960) Loss 0.00239 Acc 0.92708 | Val acc 0.94155 | Model saved to /tmp/model, global_step 1008\n",
      "INFO:tensorflow:Restoring parameters from /tmp/model-1008\n"
     ]
    }
   ],
   "source": [
    "with g1.as_default():\n",
    "    emiDriver = EMI_Driver(inputPipeline, emiLSTM, emiTrainer)\n",
    "\n",
    "emiDriver.initializeSession(g1)\n",
    "smsOut = emiDriver.run(numClasses=NUM_OUTPUT, x_train=x_train, y_train=y_train, bag_train=BAG_TRAIN, \n",
    "              x_val=x_val, y_val=y_val, bag_val=BAG_VAL, numIter=NUM_ITER,\n",
    "              numRounds=NUM_ROUNDS, batchSize=BATCH_SIZE, numEpochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-23T13:59:44.680066Z",
     "start_time": "2018-07-23T13:59:43.879365Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final step test accuracy: 0.861204\n"
     ]
    }
   ],
   "source": [
    "opList = emiTrainer.accTilda\n",
    "acc = emiDriver.runOps(opList, x_test, y_test, BATCH_SIZE)\n",
    "print(\"Final step test accuracy: %f\" % np.mean(acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
