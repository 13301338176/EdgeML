---
layout: default
title: Getting Started with EdgeML
permalink: /Algorithms
---

The algorithms that are part of EdgeML are written in Tensorflow for Python.
They are hosted on [GitHub](https://github.com/Microsoft/EdgeML/).
Additionally, the repository also provides fast and scalable C++
implementations of Bonsai and ProtoNN. A very brief introduction of these
algorithms is provided below.


## Bonsai

*Bonsai* is tree-based multi-class classification algorithm. Bonsai enables
high prediction accuracy while minimizing model size and prediction costs by a)
learning a single, shallow, sparse tree with powerful nodes, b) sparsely
projecting data into a low-dimensional space and c) jointly learning the tree
and projection parameters.

Get started with Bonsai through
<a style="color:var(--ms-green);"
href="https://github.com/Microsoft/EdgeML/tree/master/tf/examples/Bonsai">examples
</a>. Learn more about Bonsai from our
<a
href="http://manikvarma.org/pubs/kumar17.pdf"
style="color:var(--ms-green);">ICML '17 publication</a>.


## ProtoNN

*ProtoNN* is a multi-class classification algorithm, inspired by k-Nearest
Neighbor (kNN). Models generated by ProtoNN have several orders lowers storage
and prediction complexity. This is enabled by a) learning a small number of
prototypes to represent the entire training set, b) sparse low dimensional
projection of data and c) joint discriminative learning of the projection and
prototypes.

Get started with ProtoNN through
<a style="color:var(--ms-green);"
href="https://github.com/Microsoft/EdgeML/tree/master/tf/examples/ProtoNN">examples
</a>. Learn more about ProtoNN from our
<a
href="https://github.com/Microsoft/EdgeML/blob/master/docs/publications/ProtoNN.pdf"
style="color:var(--ms-green);">ICML '17 publication</a>.


## EMI-RNN

*EMI-RNN* is a Multiple Instance learning formulation for time-series data.
Early Multi Instance (EMI) RNN exploits the fact that a) *signature* of a
particular class is a small fraction of the overall data and b) class
signatures tend to be discernible early-on
to learn a model that not only enables early prediction but also improves
accuracy.

Get started with EMI-RNN through
<a style="color:var(--ms-green);"
href="https://github.com/Microsoft/EdgeML/tree/master/tf/examples/EMI-RNN">examples
</a>. Learn more about EMI-RNN from our
<a
href="https://github.com/Microsoft/EdgeML/blob/master/docs/publications/emi-rnn-nips18.pdf"
style="color:var(--ms-green);">NIPS '18 publication</a>.



## FastRNN and FastGRNN

*FastRNN* and *FastGRNN* are two novel RNN architectures (together called Fast
Cells) designed to address the twin RNN limiations of inaccurate training and 
inefficient prediction. FastRNN provably stabilizes the RNN training which 
usually suffers from vanishing and exploding gradients. FastGRNN is a gated RNN 
extended over FastRNN, that learns low-rank, sparse and quantized weight matrices 
resulting in models that are up to **35x** smaller and faster for inference compared 
to LSTM/GRU without compromising prediction accuracies.

Get started with Fast Cells through
<a style="color:var(--ms-green);"
href="https://github.com/Microsoft/EdgeML/tree/master/tf/examples/FastCells">examples.</a>
Learn more about Fast Cells from our
<a
href="http://manikvarma.org/pubs/kusupati18.pdf"
style="color:var(--ms-green);">NIPS '18 publication</a>.
